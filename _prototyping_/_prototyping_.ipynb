{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f09314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, './..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "from matplotlib.patches import Rectangle\n",
    "from math import prod\n",
    "from tqdm import tqdm\n",
    "import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e8e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_map = {\n",
    "    'signal': spark.PortSpecs(payload_type=spark.FloatArray, shape=(4,), dtype=jnp.float16)\n",
    "}\n",
    "output_map = {\n",
    "    'integrator': {\n",
    "        'signal': spark.PortSpecs(payload_type=spark.FloatArray, shape=(2,), dtype=jnp.float16)\n",
    "\t}\n",
    "}\n",
    "modules_map = {\n",
    "   'spiker': spark.ModuleSpecs(\n",
    "        name ='spiker', \n",
    "\t\tmodule_cls = spark.nn.interfaces.TopologicalLinearSpiker,\n",
    "        inputs = {\n",
    "        \t'signal': [\n",
    "                spark.PortMap(origin='__call__', port='signal'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.interfaces.TopologicalLinearSpikerConfig(\n",
    "\t\t\tglue = jnp.array(0), \n",
    "\t\t\tmins = jnp.array(-1),  \n",
    "\t\t\tmaxs = jnp.array(1), \n",
    "            resolution = 128,\n",
    "            max_freq = 200,\n",
    "            tau = 30.0,\n",
    "\t\t)\n",
    "\t),\n",
    "    'A_ex': spark.ModuleSpecs(\n",
    "        name ='A_ex', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='spikes'),\n",
    "                spark.PortMap(origin='A_ex', port='out_spikes'),\n",
    "                spark.PortMap(origin='B_in', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256+64,), \n",
    "            _s_target_units = (256,),\n",
    "            inhibitory_rate = 0.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "    'A_in': spark.ModuleSpecs(\n",
    "        name ='A_in', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='spikes'),\n",
    "                spark.PortMap(origin='A_ex', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256,), \n",
    "            _s_target_units = (64,),\n",
    "            inhibitory_rate = 1.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "\t'B_ex': spark.ModuleSpecs(\n",
    "        name ='B_ex', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='spikes'),\n",
    "                spark.PortMap(origin='B_ex', port='out_spikes'),\n",
    "                spark.PortMap(origin='A_in', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256+64,), \n",
    "            _s_target_units = (256,),\n",
    "            inhibitory_rate = 0.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "\t'B_in': spark.ModuleSpecs(\n",
    "        name ='B_in', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='spikes'),\n",
    "                spark.PortMap(origin='B_ex', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256,), \n",
    "            _s_target_units = (64,),\n",
    "            inhibitory_rate = 1.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "\t'integrator': spark.ModuleSpecs(\n",
    "        name ='integrator', \n",
    "\t\tmodule_cls = spark.nn.interfaces.ExponentialIntegrator, \n",
    "\t\tinputs = {\n",
    "        \t'spikes': [\n",
    "                spark.PortMap(origin='A_ex', port='out_spikes'),\n",
    "                spark.PortMap(origin='B_ex', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.interfaces.ExponentialIntegratorConfig(\n",
    "            num_outputs = 2\n",
    "\t\t)\n",
    "\t),\n",
    "}\n",
    "\n",
    "brain_config = spark.nn.BrainConfig(input_map=input_map, output_map=output_map, modules_map=modules_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca46759c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mul got incompatible shapes for broadcasting: (256,), (832,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m brain \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBrain(config\u001b[38;5;241m=\u001b[39mbrain_config)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Execute the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mbrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/core/module.py:65\u001b[0m, in \u001b[0;36mSparkMeta.__new__.<locals>.wrapped_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Use getattr for a safe check on the instance's `__built__` flag.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__built__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m#    # Call the _build method.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m#    self._build(*args, **kwargs)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#    raise RuntimeError(f'An error was encountered while trying to build module \"{self.name}\": {e}.') from e\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# After potentially building, execute the original __call__ logic.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m original_call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/core/module.py:249\u001b[0m, in \u001b[0;36mSparkModule._build\u001b[0;34m(self, *abc_args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m input_specs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_input_specs(bound_args\u001b[38;5;241m.\u001b[39marguments)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Build model.\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_specs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__built__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# TODO: The correct approach to build the model is through eval_shape. \u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# However, Constant and Variable are clashing with JAX tracer\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# For some reason, the tracer thinks that Constant produces a side effect \u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# when interacting with other arrays and Variable leaks.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# This probably requires extending both classes to tell them what to do with ShapeDtypeStruct \u001b[39;00m\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/nn/brain.py:255\u001b[0m, in \u001b[0;36mBrain.build\u001b[0;34m(self, input_specs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Initialize module\u001b[39;00m\n\u001b[1;32m    254\u001b[0m module: SparkModule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_specs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Update cache\u001b[39;00m\n\u001b[1;32m    257\u001b[0m out_specs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mget_output_specs()\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/core/module.py:226\u001b[0m, in \u001b[0;36mSparkModule.initialize\u001b[0;34m(self, shape, dtype, specs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected either shape and dtype or specs to be not None but got\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    223\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and specs:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspecs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Use call with the mock input.\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmock_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Reset stateful modules \u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/core/module.py:65\u001b[0m, in \u001b[0;36mSparkMeta.__new__.<locals>.wrapped_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Use getattr for a safe check on the instance's `__built__` flag.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__built__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m#    # Call the _build method.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m#    self._build(*args, **kwargs)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#    raise RuntimeError(f'An error was encountered while trying to build module \"{self.name}\": {e}.') from e\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# After potentially building, execute the original __call__ logic.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m original_call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/core/module.py:274\u001b[0m, in \u001b[0;36mSparkModule._build\u001b[0;34m(self, *abc_args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         mock_input[key] \u001b[38;5;241m=\u001b[39m [value\u001b[38;5;241m.\u001b[39mpayload_type(jnp\u001b[38;5;241m.\u001b[39mzeros(s, dtype\u001b[38;5;241m=\u001b[39mvalue\u001b[38;5;241m.\u001b[39mdtype)) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mshape]\n\u001b[0;32m--> 274\u001b[0m abc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmock_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Contruct output sepcs.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_output_specs(abc_output)\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/core/module.py:73\u001b[0m, in \u001b[0;36mSparkMeta.__new__.<locals>.wrapped_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m#    # Call the _build method.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m#    self._build(*args, **kwargs)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#    raise RuntimeError(f'An error was encountered while trying to build module \"{self.name}\": {e}.') from e\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# After potentially building, execute the original __call__ logic.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/nn/neurons/alif.py:141\u001b[0m, in \u001b[0;36mALIFNeuron.__call__\u001b[0;34m(self, in_spikes)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynapses\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m learning_rule_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Signed spikes\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_spikes\u001b[39m\u001b[38;5;124m'\u001b[39m: SpikeArray(\u001b[43msoma_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspikes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inhibition_mask\u001b[49m)\n\u001b[1;32m    142\u001b[0m }\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/jax/_src/numpy/array_methods.py:578\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    576\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 578\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/jax/_src/numpy/ufunc_api.py:179\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/jax/_src/numpy/ufuncs.py:1256\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Multiply two arrays element-wise.\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \n\u001b[1;32m   1232\u001b[0m \u001b[38;5;124;03mJAX implementation of :obj:`numpy.multiply`. This is a universal function,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;124;03m  Array([ 0, 10, 20, 30], dtype=int32)\u001b[39;00m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y)\n\u001b[0;32m-> 1256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mbitwise_and(x, y)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/jax/_src/lax/lax.py:128\u001b[0m, in \u001b[0;36m_try_broadcast_shapes\u001b[0;34m(name, *shapes)\u001b[0m\n\u001b[1;32m    126\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    129\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: mul got incompatible shapes for broadcasting: (256,), (832,)."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your configuration\n",
    "brain_config = spark.nn.BrainConfig(input_map=input_map, output_map=output_map, modules_map=modules_map)\n",
    "\n",
    "# Build the model\n",
    "brain = spark.nn.Brain(config=brain_config)\n",
    "\n",
    "# Execute the model\n",
    "brain(signal=spark.FloatArray(jnp.zeros((4,), dtype=jnp.float16)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ab355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3896bfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(brain.__class__, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d8dc45",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleSynapsesConfig' object has no attribute 'target_units'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/IPython/core/formatters.py:770\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    764\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    766\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    767\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    768\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    769\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 770\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/IPython/lib/pretty.py:394\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m _get_mro(obj_class):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_pprinters:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# printer registered in self.type_pprinters\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_pprinters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;66;03m# deferred printer\u001b[39;00m\n\u001b[1;32m    397\u001b[0m         printer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_deferred_types(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/IPython/lib/pretty.py:701\u001b[0m, in \u001b[0;36m_dict_pprinter_factory.<locals>.inner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    699\u001b[0m     p\u001b[38;5;241m.\u001b[39mpretty(key)\n\u001b[1;32m    700\u001b[0m     p\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 701\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m p\u001b[38;5;241m.\u001b[39mend_group(step, end)\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/IPython/lib/pretty.py:419\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    410\u001b[0m                     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    411\u001b[0m                     \u001b[38;5;66;03m# check if cls defines __repr__\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(_safe_getattr(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    418\u001b[0m                 ):\n\u001b[0;32m--> 419\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/IPython/lib/pretty.py:794\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m/usr/lib/python3.13/reprlib.py:21\u001b[0m, in \u001b[0;36mrecursive_repr.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43muser_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.13/reprlib.py:21\u001b[0m, in \u001b[0;36mrecursive_repr.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43muser_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.13/reprlib.py:21\u001b[0m, in \u001b[0;36mrecursive_repr.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43muser_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleSynapsesConfig' object has no attribute 'target_units'"
     ]
    }
   ],
   "source": [
    "brain_config.modules_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2222383e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSynapsesConfig(seed=1712792851, dtype=<class 'jax.numpy.float16'>, dt=1.0, target_units=(10,), async_spikes=True, kernel_initializer=SparseUniformKernelInitializerConfig(name='sparse_uniform_kernel_initializer', dtype=<class 'jax.numpy.float16'>, density=0.2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.nn.synapses.SimpleSynapsesConfig(target_units=(10,), async_spikes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "545253e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4}\n"
     ]
    }
   ],
   "source": [
    "x = {'a':1, 'b':2, 'c':3,}\n",
    "y = {'d':4}\n",
    "\n",
    "\n",
    "\n",
    "def test(**kwargs):\n",
    "    print(kwargs)\n",
    "    \n",
    "test(**{**x, **y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def run_model(graph, state, x):\n",
    "\tmodel = nnx.merge(graph, state)\n",
    "\tout, spikes = model(drive=x)\n",
    "\t_, state = nnx.split((model))\n",
    "\treturn out, spikes, state\n",
    "\n",
    "def process_obs(x):\n",
    "\t# CartPos, CartSpeed, PoleAngle, PoleAngSpeed \n",
    "\tx = x / np.array([2.4, 2.5, 0.2095, 3.5])\n",
    "\tx = np.clip(x, a_min=-1, a_max=1)\n",
    "\treturn x\n",
    "\n",
    "def compute_real_reward(x, x_prev, r_prev, terminated):\n",
    "\t# CartPos, CartSpeed, PoleAngle, PoleAngSpeed \n",
    "\tif terminated:\n",
    "\t\treturn 0\n",
    "\tr = (x_prev[0]**2 - x[0]**2) + (x_prev[2]**2 - x[2]**2)\n",
    "\tr = np.clip(0.5 * r_prev + 2 * r, a_min=-1, a_max=1)\n",
    "\treturn r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee7f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "env_name =  'CartPole-v1'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "next_obs, _ = env.reset(seed=42)\n",
    "next_obs = process_obs(next_obs)\n",
    "\n",
    "model = spark.Brain(input_map=input_map, output_map=output_map, modules_map=modules_map)\n",
    "model(drive=spark.FloatArray(jnp.zeros((4,), dtype=jnp.float16)))\n",
    "graph, state = nnx.split((model))\n",
    "#starting_kernel = model.neurons.synapses.get_flat_kernel()\n",
    "brain_steps_per_env_step = 10\n",
    "\n",
    "if False:\n",
    "\treward = 0\n",
    "\treward_array = []\n",
    "\tfor i in tqdm(range(5000)):\n",
    "\t\tprev_obs = next_obs\n",
    "\t\t# Model logic\n",
    "\t\tout, model_spikes, state = run_model(graph, state, spark.FloatArray(jnp.array(next_obs, dtype=jnp.float16)))\n",
    "\t\t# Environment logic.\n",
    "\t\tnext_action = int(np.argmax(out['integrator.output'].value))\n",
    "\t\tnext_obs, _, terminated, truncated, info = env.step(next_action)\n",
    "\t\tif terminated:\n",
    "\t\t\tnext_obs, _ = env.reset()\n",
    "\t\t\t# Flush model\n",
    "\t\t\tfor i in range(16):\n",
    "\t\t\t\t_, _, state = run_model(graph, state, spark.FloatArray(jnp.zeros_like(next_obs, dtype=jnp.float16)))\n",
    "\t\tnext_obs = process_obs(next_obs)\n",
    "\t\treward = compute_real_reward(next_obs, prev_obs, reward, terminated)\n",
    "\t\treward_array.append(reward)\n",
    "\n",
    "outs = []\n",
    "spikes = []\n",
    "obs = []\n",
    "breaks = []\n",
    "break_obs = []\n",
    "actions = []\n",
    "reward = 0\n",
    "next_obs, _ = env.reset(seed=42+1)\n",
    "next_obs = process_obs(next_obs)\n",
    "for i in tqdm(range(100)):\n",
    "\tprev_obs = next_obs\n",
    "\t# Model logic\n",
    "\tfor _ in range(brain_steps_per_env_step):\n",
    "\t\tout, model_spikes, state = run_model(graph, state, spark.FloatArray(jnp.array(next_obs, dtype=jnp.float16)))\n",
    "\t\touts.append(out['integrator.output'].value)\n",
    "\t\tspikes.append(jnp.concatenate([s.value.reshape(-1) for s in model_spikes]))\n",
    "\t\t# Environment logic.\n",
    "\t\tnext_action = int(np.argmax(out['integrator.output'].value))\n",
    "\t\tactions.append(next_action)\n",
    "\tnext_obs, _, terminated, truncated, info = env.step(next_action)\n",
    "\tif terminated:\n",
    "\t\tbreak_obs.append(next_obs)\n",
    "\t\tnext_obs, _ = env.reset()\n",
    "\t\tbreaks.append(brain_steps_per_env_step*i)\n",
    "\t\t# Flush model\n",
    "\t\tfor i in range(50):\n",
    "\t\t\t_, _, state = run_model(graph, state, spark.FloatArray(jnp.zeros_like(next_obs, dtype=jnp.float16)))\n",
    "\tnext_obs = process_obs(next_obs)\n",
    "\treward = compute_real_reward(next_obs, prev_obs, reward, terminated)\n",
    "\tobs.append(next_obs)\n",
    "\t\n",
    "model = nnx.merge(graph, state)\n",
    "\n",
    "spikes = np.abs(np.array(spikes))\n",
    "fig, ax = plt.subplots(2,1,figsize=(20,10), height_ratios=(8,2))\n",
    "ax[0].imshow(1-spikes.T, cmap='gray', aspect='auto', interpolation='none')\n",
    "for b in breaks:\n",
    "    ax[0].plot([b,b], [0-0.5,len(spikes)-0.5], 'r--', alpha=0.1)\n",
    "for i in range(3):\n",
    "    ax[0].plot([0-0.5,len(spikes)-0.5], [128*(i+1), 128*(i+1)], 'g--', alpha=0.1)\n",
    "ax[0].plot(brain_steps_per_env_step*np.arange(len(spikes)//brain_steps_per_env_step), 64*np.array(obs).T[0]+64, alpha=0.4)\n",
    "ax[0].plot(brain_steps_per_env_step*np.arange(len(spikes)//brain_steps_per_env_step), 64*np.array(obs).T[1]+64+128, alpha=0.4)\n",
    "ax[0].plot(brain_steps_per_env_step*np.arange(len(spikes)//brain_steps_per_env_step), 64*np.array(obs).T[2]+64+256, alpha=0.4)\n",
    "ax[0].plot(brain_steps_per_env_step*np.arange(len(spikes)//brain_steps_per_env_step), 64*np.array(obs).T[3]+64+128+256, alpha=0.4)\n",
    "ax[1].plot(actions)\n",
    "ax[1].set_xlim(0, len(actions))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "if False:\n",
    "\tplt.imshow(starting_kernel, aspect='auto', interpolation='none')\n",
    "\tplt.colorbar()\n",
    "\tplt.show()\n",
    "\tplt.imshow(final_kernel, aspect='auto', interpolation='none')\n",
    "\tplt.colorbar()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9763025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add QT to the update loop. \n",
    "# Makes the editor non-blocking.\n",
    "%gui qt\n",
    "import sys\n",
    "sys.path.insert(1, './..')\n",
    "\n",
    "import spark\n",
    "editor = spark.SparkGraphEditor()\n",
    "# Start editor on the main thread.\n",
    "if __name__ == '__main__':\n",
    "\teditor.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc56e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_map, output_map, modules_map = editor.compile_model()\n",
    "model_def = {\n",
    "\t'input_map': input_map,\n",
    "\t'output_map': output_map,\n",
    "\t'modules_map': modules_map,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark.core.specs import ModuleSpecs, PortSpecs, PortMap\n",
    "from spark.core.registry import REGISTRY\n",
    "from spark.core.module import SparkModule\n",
    "import typing as tp\n",
    "import copy\n",
    "\n",
    "def from_json(model_json: dict[str, dict]) -> dict[str, tp.Any]:\n",
    "\t# Deepcopy to prevent overrides to original JSON.\n",
    "\tmodel_json = copy.deepcopy(model_json)\n",
    "\t# Reconstruct input_map\n",
    "\tinput_map: dict[str, PortSpecs] = {}\n",
    "\tfor name, map in model_json['input_map'].items():\n",
    "\t\tinput_map[name] = PortSpecs(**map) \n",
    "\t# Reconstruct output_map\n",
    "\toutput_map: dict[str, dict[str, PortSpecs]] = {}\n",
    "\tfor name, map in model_json['output_map'].items():\n",
    "\t\tport_map = map.pop('port_maps')[0]\n",
    "\t\tmap.pop('is_optional')\n",
    "\t\torigin, port_name = port_map['origin'], port_map['port']\n",
    "\t\tif not origin in output_map:\n",
    "\t\t\toutput_map[origin] = {}\n",
    "\t\toutput_map[origin][port_name] = PortSpecs(**map) \n",
    "\t# Reconstruct modules_map\n",
    "\tmodules_map: dict[str, dict[str, spark.ModuleSpecs]] = {}\n",
    "\tfor name, map in model_json['modules_map'].items():\n",
    "\t\tclass_ref: type[SparkModule] = REGISTRY.MODULES.get(map['module_cls']).class_ref\n",
    "\t\tport_maps: dict[str, list[spark.PortMap]] = {}\n",
    "\t\tfor input_port_name in map['inputs']:\n",
    "\t\t\tport_maps[input_port_name] = [\n",
    "\t\t\t\tspark.PortMap(origin=pm['origin'], port=pm['port']) for pm in map['inputs'][input_port_name]\n",
    "\t\t\t]\n",
    "\t\tmodules_map[name] = spark.ModuleSpecs(\n",
    "\t\t\tname = name,\n",
    "\t\t\tmodule_cls = class_ref,\n",
    "\t\t\tinputs = port_maps,\n",
    "\t\t\tconfig = class_ref.get_default_config_class()(**map['config']),\n",
    "\t\t)\n",
    "\treturn {\n",
    "\t\t'input_map': input_map, \n",
    "\t\t'output_map': output_map,\n",
    "\t\t'modules_map': modules_map,\n",
    "\t}\n",
    "\n",
    "from_json(model_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = f'abc {1}' \\\n",
    "\tf'fgh {2}'\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
