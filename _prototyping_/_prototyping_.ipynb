{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f09314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, './..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "from matplotlib.patches import Rectangle\n",
    "from math import prod\n",
    "from tqdm import tqdm\n",
    "import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e8e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_map = {\n",
    "    'drive': spark.PortSpecs(payload_type=spark.FloatArray, shape=(4,), dtype=jnp.float16)\n",
    "}\n",
    "output_map = {\n",
    "    'integrator': {\n",
    "        'output': spark.PortSpecs(payload_type=spark.FloatArray, shape=(2,), dtype=jnp.float16)\n",
    "\t}\n",
    "}\n",
    "modules_map = {\n",
    "   'spiker': spark.ModuleSpecs(\n",
    "        name ='spiker', \n",
    "\t\tmodule_cls = spark.nn.interfaces.TopologicalLinearSpiker,\n",
    "        inputs = {\n",
    "        \t'drive': [\n",
    "                spark.PortMap(origin='__call__', port='drive'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.interfaces.TopologicalLinearSpikerConfig(\n",
    "\t\t\tglue = jnp.array(0), \n",
    "\t\t\tmins = jnp.array(-1),  \n",
    "\t\t\tmaxs = jnp.array(1), \n",
    "            resolution = 128,\n",
    "            max_freq = 200,\n",
    "            tau = 30.0,\n",
    "\t\t)\n",
    "\t),\n",
    "    'A_ex': spark.ModuleSpecs(\n",
    "        name ='A_ex', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='out_spikes'),\n",
    "                spark.PortMap(origin='A_ex', port='out_spikes'),\n",
    "                spark.PortMap(origin='B_in', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256+64,), \n",
    "            _s_target_units = (256,),\n",
    "            inhibitory_rate = 0.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "    'A_in': spark.ModuleSpecs(\n",
    "        name ='A_in', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='out_spikes'),\n",
    "                spark.PortMap(origin='A_ex', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256,), \n",
    "            _s_target_units = (64,),\n",
    "            inhibitory_rate = 1.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "\t'B_ex': spark.ModuleSpecs(\n",
    "        name ='B_ex', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='out_spikes'),\n",
    "                spark.PortMap(origin='B_ex', port='out_spikes'),\n",
    "                spark.PortMap(origin='A_in', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256+64,), \n",
    "            _s_target_units = (256,),\n",
    "            inhibitory_rate = 0.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "\t'B_in': spark.ModuleSpecs(\n",
    "        name ='B_in', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='out_spikes'),\n",
    "                spark.PortMap(origin='B_ex', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256,), \n",
    "            _s_target_units = (64,),\n",
    "            inhibitory_rate = 1.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "\t'integrator': spark.ModuleSpecs(\n",
    "        name ='integrator', \n",
    "\t\tmodule_cls = spark.nn.interfaces.ExponentialIntegrator, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='A_ex', port='out_spikes'),\n",
    "                spark.PortMap(origin='B_ex', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.interfaces.ExponentialIntegratorConfig(\n",
    "            num_outputs = 2\n",
    "\t\t)\n",
    "\t),\n",
    "}\n",
    "\n",
    "brain_config = spark.nn.BrainConfig(input_map=input_map, output_map=output_map, modules_map=modules_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f6b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_map = {\n",
    "    'drive': spark.PortSpecs(payload_type=spark.FloatArray, shape=(4,), dtype=jnp.float16)\n",
    "}\n",
    "output_map = {\n",
    "    'A_ex': {\n",
    "        'out_spikes': spark.PortSpecs(payload_type=spark.FloatArray, shape=(2,), dtype=jnp.float16)\n",
    "\t}\n",
    "}\n",
    "modules_map = {\n",
    "   'spiker': spark.ModuleSpecs(\n",
    "        name ='spiker', \n",
    "\t\tmodule_cls = spark.nn.interfaces.TopologicalLinearSpiker,\n",
    "        inputs = {\n",
    "        \t'drive': [\n",
    "                spark.PortMap(origin='__call__', port='drive'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.interfaces.TopologicalLinearSpikerConfig(\n",
    "\t\t\tglue = jnp.array(0), \n",
    "\t\t\tmins = jnp.array(-1),  \n",
    "\t\t\tmaxs = jnp.array(1), \n",
    "            resolution = 128,\n",
    "            max_freq = 200,\n",
    "            tau = 30.0,\n",
    "\t\t)\n",
    "\t),\n",
    "    'A_ex': spark.ModuleSpecs(\n",
    "        name ='A_ex', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='out_spikes'),\n",
    "                spark.PortMap(origin='A_ex', port='out_spikes'),\n",
    "                spark.PortMap(origin='B_in', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256+64,), \n",
    "            _s_target_units = (256,),\n",
    "            inhibitory_rate = 0.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "    'A_in': spark.ModuleSpecs(\n",
    "        name ='A_in', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='out_spikes'),\n",
    "                spark.PortMap(origin='A_ex', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256,), \n",
    "            _s_target_units = (64,),\n",
    "            inhibitory_rate = 1.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "\t'B_ex': spark.ModuleSpecs(\n",
    "        name ='B_ex', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='out_spikes'),\n",
    "                spark.PortMap(origin='B_ex', port='out_spikes'),\n",
    "                spark.PortMap(origin='A_in', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256+64,), \n",
    "            _s_target_units = (256,),\n",
    "            inhibitory_rate = 0.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "\t'B_in': spark.ModuleSpecs(\n",
    "        name ='B_in', \n",
    "\t\tmodule_cls = spark.nn.neurons.ALIFNeuron, \n",
    "\t\tinputs = {\n",
    "        \t'in_spikes': [\n",
    "                spark.PortMap(origin='spiker', port='out_spikes'),\n",
    "                spark.PortMap(origin='B_ex', port='out_spikes'),\n",
    "        \t]\n",
    "    \t},\n",
    "\t\tconfig = spark.nn.neurons.ALIFNeuronConfig(\n",
    "\t\t\tunits = (4*128+256,), \n",
    "            _s_target_units = (64,),\n",
    "            inhibitory_rate = 1.0,\n",
    "\t\t\t_s_async_spikes = True\n",
    "\t\t)\n",
    "\t),\n",
    "}\n",
    "\n",
    "brain_config = spark.nn.BrainConfig(input_map=input_map, output_map=output_map, modules_map=modules_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': {'drive': FloatArray(value=Array([0., 0., 0., 0.], dtype=float16))}}\n",
      "{'drive': FloatArray(value=Array([0., 0., 0., 0.], dtype=float16))}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m brain \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBrain(config\u001b[38;5;241m=\u001b[39mbrain_config)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/core/module.py:65\u001b[0m, in \u001b[0;36mSparkMeta.__new__.<locals>.wrapped_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Use getattr for a safe check on the instance's `__built__` flag.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__built__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m#    # Call the _build method.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m#    self._build(*args, **kwargs)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#    raise RuntimeError(f'An error was encountered while trying to build module \"{self.name}\": {e}.') from e\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# After potentially building, execute the original __call__ logic.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m original_call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/core/module.py:247\u001b[0m, in \u001b[0;36mSparkModule._build\u001b[0;34m(self, *abc_args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError binding arguments for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Construct input specs.\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m input_specs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_input_specs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Build model.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(input_specs)\n",
      "File \u001b[0;32m~/Project/Spark/_prototyping_/../spark/core/module.py:370\u001b[0m, in \u001b[0;36mSparkModule._construct_input_specs\u001b[0;34m(self, abc_args)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m abc_paylaod:\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28mprint\u001b[39m(abc_paylaod)\n\u001b[0;32m--> 370\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[43mabc_paylaod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(abc_paylaod, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [p\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m abc_paylaod]\n\u001b[1;32m    371\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m abc_paylaod\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(abc_paylaod, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m abc_paylaod[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_specs[key], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m, shape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your configuration\n",
    "brain_config = spark.nn.BrainConfig(input_map=input_map, output_map=output_map, modules_map=modules_map)\n",
    "\n",
    "# Build the model\n",
    "brain = spark.nn.Brain(config=brain_config)\n",
    "\n",
    "# Execute the model\n",
    "brain(drive=spark.FloatArray(jnp.zeros((4,), dtype=jnp.float16)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba474c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BrainConfig(seed=2160282351, dtype=<class 'jax.numpy.float16'>, dt=1.0, input_map={'drive': PortSpecs(payload_type=<class 'spark.core.payloads.FloatArray'>, shape=(4,), dtype=<class 'jax.numpy.float16'>, description=None)}, output_map={'integrator': {'output': PortSpecs(payload_type=<class 'spark.core.payloads.FloatArray'>, shape=(2,), dtype=<class 'jax.numpy.float16'>, description=None)}}, modules_map={'spiker': ModuleSpecs(name='integrator', module_cls=<class 'spark.nn.interfaces.input.TopologicalLinearSpiker'>, inputs={'drive': [PortMap(origin='__call__', port='drive')]}, config=TopologicalLinearSpikerConfig(seed=1624172830, dtype=<class 'jax.numpy.float16'>, dt=1.0, tau=30.0, cd=2.0, max_freq=200, glue=Array(0, dtype=int32, weak_type=True), mins=Array(-1, dtype=int32, weak_type=True), maxs=Array(1, dtype=int32, weak_type=True), resolution=128, sigma=0.03125)), 'A_ex': ModuleSpecs(name='A_ex', module_cls=<class 'spark.nn.neurons.alif.ALIFNeuron'>, inputs={'in_spikes': [PortMap(origin='spiker', port='out_spikes'), PortMap(origin='A_ex', port='out_spikes'), PortMap(origin='B_in', port='out_spikes')]}, config=ALIFNeuronConfig(seed=668560008, dtype=<class 'jax.numpy.float16'>, dt=1.0, units=(832,), max_delay=0.2, inhibitory_rate=0.0, async_spikes=True, soma_config=ALIFSomaConfig(seed=2822549177, dtype=<class 'jax.numpy.float16'>, dt=1.0, potential_rest=-60.0, potential_reset=-50.0, potential_tau=20.0, conductance=10.0, threshold=-40.0, threshold_tau=100.0, threshold_delta=100.0, cooldown=2.0), synapses_config=SimpleSynapsesConfig(seed=4021955294, dtype=<class 'jax.numpy.float16'>, dt=1.0, target_units=(256,), async_spikes=True, kernel_initializer=SparseUniformKernelInitializerConfig(name='sparse_uniform_kernel_initializer', dtype=<class 'jax.numpy.float16'>, density=0.2)), delays_config=N2NDelaysConfig(seed=906153649, dtype=<class 'jax.numpy.float16'>, dt=1.0, max_delay=8.0, delay_initializer=UniformDelayInitializerConfig(name='uniform_delay_initializer', dtype=<class 'jax.numpy.uint8'>), target_units=(256,)), learning_rule_config=HebbianLearningConfig(seed=1919417283, dtype=<class 'jax.numpy.float16'>, dt=1.0, async_spikes=True, pre_tau=20.0, post_tau=20.0, post_slow_tau=20.0, target_tau=20000.0, a=1.0, b=-1.0, c=-1.0, d=1.0, p=20.0, gamma=0.1))), 'A_in': ModuleSpecs(name='A_in', module_cls=<class 'spark.nn.neurons.alif.ALIFNeuron'>, inputs={'in_spikes': [PortMap(origin='spiker', port='out_spikes'), PortMap(origin='A_ex', port='out_spikes')]}, config=ALIFNeuronConfig(seed=2052659269, dtype=<class 'jax.numpy.float16'>, dt=1.0, units=(768,), max_delay=0.2, inhibitory_rate=1.0, async_spikes=True, soma_config=ALIFSomaConfig(seed=3221641748, dtype=<class 'jax.numpy.float16'>, dt=1.0, potential_rest=-60.0, potential_reset=-50.0, potential_tau=20.0, conductance=10.0, threshold=-40.0, threshold_tau=100.0, threshold_delta=100.0, cooldown=2.0), synapses_config=SimpleSynapsesConfig(seed=3520474847, dtype=<class 'jax.numpy.float16'>, dt=1.0, target_units=(64,), async_spikes=True, kernel_initializer=SparseUniformKernelInitializerConfig(name='sparse_uniform_kernel_initializer', dtype=<class 'jax.numpy.float16'>, density=0.2)), delays_config=N2NDelaysConfig(seed=2051768284, dtype=<class 'jax.numpy.float16'>, dt=1.0, max_delay=8.0, delay_initializer=UniformDelayInitializerConfig(name='uniform_delay_initializer', dtype=<class 'jax.numpy.uint8'>), target_units=(64,)), learning_rule_config=HebbianLearningConfig(seed=1513796838, dtype=<class 'jax.numpy.float16'>, dt=1.0, async_spikes=True, pre_tau=20.0, post_tau=20.0, post_slow_tau=20.0, target_tau=20000.0, a=1.0, b=-1.0, c=-1.0, d=1.0, p=20.0, gamma=0.1))), 'B_ex': ModuleSpecs(name='B_ex', module_cls=<class 'spark.nn.neurons.alif.ALIFNeuron'>, inputs={'in_spikes': [PortMap(origin='spiker', port='out_spikes'), PortMap(origin='B_ex', port='out_spikes'), PortMap(origin='A_in', port='out_spikes')]}, config=ALIFNeuronConfig(seed=461607576, dtype=<class 'jax.numpy.float16'>, dt=1.0, units=(832,), max_delay=0.2, inhibitory_rate=0.0, async_spikes=True, soma_config=ALIFSomaConfig(seed=4137684403, dtype=<class 'jax.numpy.float16'>, dt=1.0, potential_rest=-60.0, potential_reset=-50.0, potential_tau=20.0, conductance=10.0, threshold=-40.0, threshold_tau=100.0, threshold_delta=100.0, cooldown=2.0), synapses_config=SimpleSynapsesConfig(seed=1154002432, dtype=<class 'jax.numpy.float16'>, dt=1.0, target_units=(256,), async_spikes=True, kernel_initializer=SparseUniformKernelInitializerConfig(name='sparse_uniform_kernel_initializer', dtype=<class 'jax.numpy.float16'>, density=0.2)), delays_config=N2NDelaysConfig(seed=206888238, dtype=<class 'jax.numpy.float16'>, dt=1.0, max_delay=8.0, delay_initializer=UniformDelayInitializerConfig(name='uniform_delay_initializer', dtype=<class 'jax.numpy.uint8'>), target_units=(256,)), learning_rule_config=HebbianLearningConfig(seed=1257044740, dtype=<class 'jax.numpy.float16'>, dt=1.0, async_spikes=True, pre_tau=20.0, post_tau=20.0, post_slow_tau=20.0, target_tau=20000.0, a=1.0, b=-1.0, c=-1.0, d=1.0, p=20.0, gamma=0.1))), 'B_in': ModuleSpecs(name='B_in', module_cls=<class 'spark.nn.neurons.alif.ALIFNeuron'>, inputs={'in_spikes': [PortMap(origin='spiker', port='out_spikes'), PortMap(origin='B_ex', port='out_spikes')]}, config=ALIFNeuronConfig(seed=1285237987, dtype=<class 'jax.numpy.float16'>, dt=1.0, units=(768,), max_delay=0.2, inhibitory_rate=1.0, async_spikes=True, soma_config=ALIFSomaConfig(seed=896868456, dtype=<class 'jax.numpy.float16'>, dt=1.0, potential_rest=-60.0, potential_reset=-50.0, potential_tau=20.0, conductance=10.0, threshold=-40.0, threshold_tau=100.0, threshold_delta=100.0, cooldown=2.0), synapses_config=SimpleSynapsesConfig(seed=2654912934, dtype=<class 'jax.numpy.float16'>, dt=1.0, target_units=(64,), async_spikes=True, kernel_initializer=SparseUniformKernelInitializerConfig(name='sparse_uniform_kernel_initializer', dtype=<class 'jax.numpy.float16'>, density=0.2)), delays_config=N2NDelaysConfig(seed=1626006044, dtype=<class 'jax.numpy.float16'>, dt=1.0, max_delay=8.0, delay_initializer=UniformDelayInitializerConfig(name='uniform_delay_initializer', dtype=<class 'jax.numpy.uint8'>), target_units=(64,)), learning_rule_config=HebbianLearningConfig(seed=1352524116, dtype=<class 'jax.numpy.float16'>, dt=1.0, async_spikes=True, pre_tau=20.0, post_tau=20.0, post_slow_tau=20.0, target_tau=20000.0, a=1.0, b=-1.0, c=-1.0, d=1.0, p=20.0, gamma=0.1))), 'integrator': ModuleSpecs(name='integrator', module_cls=<class 'spark.nn.interfaces.output.ExponentialIntegrator'>, inputs={'in_spikes': [PortMap(origin='A_ex', port='out_spikes'), PortMap(origin='B_ex', port='out_spikes')]}, config=ExponentialIntegratorConfig(seed=2629757013, dtype=<class 'jax.numpy.float16'>, dt=1.0, num_outputs=2, saturation_freq=50.0, tau=20.0, shuffle=True, smooth_trace=True))})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d8dc45",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleSynapsesConfig' object has no attribute 'target_units'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/IPython/core/formatters.py:770\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    764\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    766\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    767\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    768\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    769\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 770\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/IPython/lib/pretty.py:394\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m _get_mro(obj_class):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_pprinters:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# printer registered in self.type_pprinters\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_pprinters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;66;03m# deferred printer\u001b[39;00m\n\u001b[1;32m    397\u001b[0m         printer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_deferred_types(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/IPython/lib/pretty.py:701\u001b[0m, in \u001b[0;36m_dict_pprinter_factory.<locals>.inner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    699\u001b[0m     p\u001b[38;5;241m.\u001b[39mpretty(key)\n\u001b[1;32m    700\u001b[0m     p\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 701\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m p\u001b[38;5;241m.\u001b[39mend_group(step, end)\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/IPython/lib/pretty.py:419\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    410\u001b[0m                     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    411\u001b[0m                     \u001b[38;5;66;03m# check if cls defines __repr__\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(_safe_getattr(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    418\u001b[0m                 ):\n\u001b[0;32m--> 419\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Project/Spark/.env/lib/python3.13/site-packages/IPython/lib/pretty.py:794\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m/usr/lib/python3.13/reprlib.py:21\u001b[0m, in \u001b[0;36mrecursive_repr.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43muser_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.13/reprlib.py:21\u001b[0m, in \u001b[0;36mrecursive_repr.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43muser_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.13/reprlib.py:21\u001b[0m, in \u001b[0;36mrecursive_repr.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43muser_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleSynapsesConfig' object has no attribute 'target_units'"
     ]
    }
   ],
   "source": [
    "brain_config.modules_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2222383e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSynapsesConfig(seed=1712792851, dtype=<class 'jax.numpy.float16'>, dt=1.0, target_units=(10,), async_spikes=True, kernel_initializer=SparseUniformKernelInitializerConfig(name='sparse_uniform_kernel_initializer', dtype=<class 'jax.numpy.float16'>, density=0.2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.nn.synapses.SimpleSynapsesConfig(target_units=(10,), async_spikes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "545253e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4}\n"
     ]
    }
   ],
   "source": [
    "x = {'a':1, 'b':2, 'c':3,}\n",
    "y = {'d':4}\n",
    "\n",
    "\n",
    "\n",
    "def test(**kwargs):\n",
    "    print(kwargs)\n",
    "    \n",
    "test(**{**x, **y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def run_model(graph, state, x):\n",
    "\tmodel = nnx.merge(graph, state)\n",
    "\tout, spikes = model(drive=x)\n",
    "\t_, state = nnx.split((model))\n",
    "\treturn out, spikes, state\n",
    "\n",
    "def process_obs(x):\n",
    "\t# CartPos, CartSpeed, PoleAngle, PoleAngSpeed \n",
    "\tx = x / np.array([2.4, 2.5, 0.2095, 3.5])\n",
    "\tx = np.clip(x, a_min=-1, a_max=1)\n",
    "\treturn x\n",
    "\n",
    "def compute_real_reward(x, x_prev, r_prev, terminated):\n",
    "\t# CartPos, CartSpeed, PoleAngle, PoleAngSpeed \n",
    "\tif terminated:\n",
    "\t\treturn 0\n",
    "\tr = (x_prev[0]**2 - x[0]**2) + (x_prev[2]**2 - x[2]**2)\n",
    "\tr = np.clip(0.5 * r_prev + 2 * r, a_min=-1, a_max=1)\n",
    "\treturn r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee7f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "env_name =  'CartPole-v1'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "next_obs, _ = env.reset(seed=42)\n",
    "next_obs = process_obs(next_obs)\n",
    "\n",
    "model = spark.Brain(input_map=input_map, output_map=output_map, modules_map=modules_map)\n",
    "model(drive=spark.FloatArray(jnp.zeros((4,), dtype=jnp.float16)))\n",
    "graph, state = nnx.split((model))\n",
    "#starting_kernel = model.neurons.synapses.get_flat_kernel()\n",
    "brain_steps_per_env_step = 10\n",
    "\n",
    "if False:\n",
    "\treward = 0\n",
    "\treward_array = []\n",
    "\tfor i in tqdm(range(5000)):\n",
    "\t\tprev_obs = next_obs\n",
    "\t\t# Model logic\n",
    "\t\tout, model_spikes, state = run_model(graph, state, spark.FloatArray(jnp.array(next_obs, dtype=jnp.float16)))\n",
    "\t\t# Environment logic.\n",
    "\t\tnext_action = int(np.argmax(out['integrator.output'].value))\n",
    "\t\tnext_obs, _, terminated, truncated, info = env.step(next_action)\n",
    "\t\tif terminated:\n",
    "\t\t\tnext_obs, _ = env.reset()\n",
    "\t\t\t# Flush model\n",
    "\t\t\tfor i in range(16):\n",
    "\t\t\t\t_, _, state = run_model(graph, state, spark.FloatArray(jnp.zeros_like(next_obs, dtype=jnp.float16)))\n",
    "\t\tnext_obs = process_obs(next_obs)\n",
    "\t\treward = compute_real_reward(next_obs, prev_obs, reward, terminated)\n",
    "\t\treward_array.append(reward)\n",
    "\n",
    "outs = []\n",
    "spikes = []\n",
    "obs = []\n",
    "breaks = []\n",
    "break_obs = []\n",
    "actions = []\n",
    "reward = 0\n",
    "next_obs, _ = env.reset(seed=42+1)\n",
    "next_obs = process_obs(next_obs)\n",
    "for i in tqdm(range(100)):\n",
    "\tprev_obs = next_obs\n",
    "\t# Model logic\n",
    "\tfor _ in range(brain_steps_per_env_step):\n",
    "\t\tout, model_spikes, state = run_model(graph, state, spark.FloatArray(jnp.array(next_obs, dtype=jnp.float16)))\n",
    "\t\touts.append(out['integrator.output'].value)\n",
    "\t\tspikes.append(jnp.concatenate([s.value.reshape(-1) for s in model_spikes]))\n",
    "\t\t# Environment logic.\n",
    "\t\tnext_action = int(np.argmax(out['integrator.output'].value))\n",
    "\t\tactions.append(next_action)\n",
    "\tnext_obs, _, terminated, truncated, info = env.step(next_action)\n",
    "\tif terminated:\n",
    "\t\tbreak_obs.append(next_obs)\n",
    "\t\tnext_obs, _ = env.reset()\n",
    "\t\tbreaks.append(brain_steps_per_env_step*i)\n",
    "\t\t# Flush model\n",
    "\t\tfor i in range(50):\n",
    "\t\t\t_, _, state = run_model(graph, state, spark.FloatArray(jnp.zeros_like(next_obs, dtype=jnp.float16)))\n",
    "\tnext_obs = process_obs(next_obs)\n",
    "\treward = compute_real_reward(next_obs, prev_obs, reward, terminated)\n",
    "\tobs.append(next_obs)\n",
    "\t\n",
    "model = nnx.merge(graph, state)\n",
    "\n",
    "spikes = np.abs(np.array(spikes))\n",
    "fig, ax = plt.subplots(2,1,figsize=(20,10), height_ratios=(8,2))\n",
    "ax[0].imshow(1-spikes.T, cmap='gray', aspect='auto', interpolation='none')\n",
    "for b in breaks:\n",
    "    ax[0].plot([b,b], [0-0.5,len(spikes)-0.5], 'r--', alpha=0.1)\n",
    "for i in range(3):\n",
    "    ax[0].plot([0-0.5,len(spikes)-0.5], [128*(i+1), 128*(i+1)], 'g--', alpha=0.1)\n",
    "ax[0].plot(brain_steps_per_env_step*np.arange(len(spikes)//brain_steps_per_env_step), 64*np.array(obs).T[0]+64, alpha=0.4)\n",
    "ax[0].plot(brain_steps_per_env_step*np.arange(len(spikes)//brain_steps_per_env_step), 64*np.array(obs).T[1]+64+128, alpha=0.4)\n",
    "ax[0].plot(brain_steps_per_env_step*np.arange(len(spikes)//brain_steps_per_env_step), 64*np.array(obs).T[2]+64+256, alpha=0.4)\n",
    "ax[0].plot(brain_steps_per_env_step*np.arange(len(spikes)//brain_steps_per_env_step), 64*np.array(obs).T[3]+64+128+256, alpha=0.4)\n",
    "ax[1].plot(actions)\n",
    "ax[1].set_xlim(0, len(actions))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "if False:\n",
    "\tplt.imshow(starting_kernel, aspect='auto', interpolation='none')\n",
    "\tplt.colorbar()\n",
    "\tplt.show()\n",
    "\tplt.imshow(final_kernel, aspect='auto', interpolation='none')\n",
    "\tplt.colorbar()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9763025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add QT to the update loop. \n",
    "# Makes the editor non-blocking.\n",
    "%gui qt\n",
    "import sys\n",
    "sys.path.insert(1, './..')\n",
    "\n",
    "import spark\n",
    "editor = spark.SparkGraphEditor()\n",
    "# Start editor on the main thread.\n",
    "if __name__ == '__main__':\n",
    "\teditor.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc56e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_map, output_map, modules_map = editor.compile_model()\n",
    "model_def = {\n",
    "\t'input_map': input_map,\n",
    "\t'output_map': output_map,\n",
    "\t'modules_map': modules_map,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark.core.specs import ModuleSpecs, PortSpecs, PortMap\n",
    "from spark.core.registry import REGISTRY\n",
    "from spark.core.module import SparkModule\n",
    "import typing as tp\n",
    "import copy\n",
    "\n",
    "def from_json(model_json: dict[str, dict]) -> dict[str, tp.Any]:\n",
    "\t# Deepcopy to prevent overrides to original JSON.\n",
    "\tmodel_json = copy.deepcopy(model_json)\n",
    "\t# Reconstruct input_map\n",
    "\tinput_map: dict[str, PortSpecs] = {}\n",
    "\tfor name, map in model_json['input_map'].items():\n",
    "\t\tinput_map[name] = PortSpecs(**map) \n",
    "\t# Reconstruct output_map\n",
    "\toutput_map: dict[str, dict[str, PortSpecs]] = {}\n",
    "\tfor name, map in model_json['output_map'].items():\n",
    "\t\tport_map = map.pop('port_maps')[0]\n",
    "\t\tmap.pop('is_optional')\n",
    "\t\torigin, port_name = port_map['origin'], port_map['port']\n",
    "\t\tif not origin in output_map:\n",
    "\t\t\toutput_map[origin] = {}\n",
    "\t\toutput_map[origin][port_name] = PortSpecs(**map) \n",
    "\t# Reconstruct modules_map\n",
    "\tmodules_map: dict[str, dict[str, spark.ModuleSpecs]] = {}\n",
    "\tfor name, map in model_json['modules_map'].items():\n",
    "\t\tclass_ref: type[SparkModule] = REGISTRY.MODULES.get(map['module_cls']).class_ref\n",
    "\t\tport_maps: dict[str, list[spark.PortMap]] = {}\n",
    "\t\tfor input_port_name in map['inputs']:\n",
    "\t\t\tport_maps[input_port_name] = [\n",
    "\t\t\t\tspark.PortMap(origin=pm['origin'], port=pm['port']) for pm in map['inputs'][input_port_name]\n",
    "\t\t\t]\n",
    "\t\tmodules_map[name] = spark.ModuleSpecs(\n",
    "\t\t\tname = name,\n",
    "\t\t\tmodule_cls = class_ref,\n",
    "\t\t\tinputs = port_maps,\n",
    "\t\t\tconfig = class_ref.get_default_config_class()(**map['config']),\n",
    "\t\t)\n",
    "\treturn {\n",
    "\t\t'input_map': input_map, \n",
    "\t\t'output_map': output_map,\n",
    "\t\t'modules_map': modules_map,\n",
    "\t}\n",
    "\n",
    "from_json(model_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = f'abc {1}' \\\n",
    "\tf'fgh {2}'\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
