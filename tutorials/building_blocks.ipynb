{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial \\#1: Modules and Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Spark</i></b> is intended as a research framework. As such one of the first things that one may want is to add a new element to the systems. <br>\n",
    "<br>\n",
    "Fortunately, adding new components is quite straight forward, although it may be a little confusing the first time. Let's start from the beginning. In <b><i>Spark</i></b>, as in any other major library, every module inherits from <b>spark.nn.Module</b>. This class is at the heart of most of the <b><i>Spark</i></b>'s magic. Depending on the specificity of the module you are trying to build it could make sense to subclass other classes:\n",
    "\n",
    "| Name | Purpose |\n",
    "| --- | --- |\n",
    "| spark.nn.Component | Arbitrary component of a neuron |\n",
    "| spark.nn.somas.Soma | Soma models |\n",
    "| spark.nn.synapses.Synanpses | Synaptic models |\n",
    "| spark.nn.learning_rules.LearningRule | Learning rules |\n",
    "| spark.nn.delays.Delays | Spike delay mechanisms |\n",
    "\n",
    "This list is not exhaustive, but it contains the most important subclasses of <b>spark.nn.Module</b>. \n",
    "\n",
    "<b> REMARK: </b>One important thing to mention before we start is that Python does not strictly enforce typing, however, Spark does rely on typing for a few of our core features. Although some of your code may work without enforcing typing there are a few things that will require it if you like for your components to play nicely with the Spark ecosystem. In general, it is quite straight forward and is only required in a few parts, so if you really despise typing most of your code can still be typeless.\n",
    "\n",
    "For the time being let's just create a simple module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(1, './..')\n",
    "\n",
    "import spark\n",
    "import typing as tp\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things to notice is new components are defined in pairs of Module - Configurations, where the Module defines the logic and the Configurations defines all the parameters required to initialize that Module. By convention we simply name these pairs as Module - ModuleConfig. \n",
    "\n",
    "Every time a new module is defined we need to link it to its default configuration. This is done by simply adding  <b>config: ConfigClass</b> to the Module class definition.\n",
    "\n",
    "Another thing to notice at this point is that the signature of all the init methods is common across all modules, it should only accept a configuration class, which by default can be empty and keywords arguments. Similarly, the first thing to do in any <b>\\_\\_init\\_\\_</b> method is to invoke the <b>\\_\\_init\\_\\_</b> method of the super class with the pattern indicated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeModuleConfig(spark.nn.Config):\n",
    "    pass\n",
    "\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "    config: MyAwesomeModuleConfig       # <--- Default configuration class MUST always be indicated\n",
    "\n",
    "    def __init__(self, config: MyAwesomeModuleConfig = None, **kwargs):\n",
    "        super().__init__(config=config, **kwargs)       # <--- super().__init__ should always be invoked first as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you will be wondering where are all your important init arguments go? The answer is the configuration class.<br>\n",
    "<br>\n",
    "Spark modules separate model definition from model logic. This allow us to do some neat tricks on the back and it is extremely useful for reproducibility. Configuration classes should always be typed, this is done using the notation <b>variable_name: variable_type = default_value</b>. \n",
    "\n",
    "All the variables defined inside the configuration class are available in the <b>\\_\\_init\\_\\_</b> method, after calling <b>super().\\_\\_init\\_\\_</b>, under the namespace <b>self.config</b>. \n",
    "\n",
    "Another important thing to notice here is that we cannot store arrays directly. Every array, must be properly wrapper within a <b>spark.Constant</b> or a <b>spark.Variable</b>. This wrappers are necessary when we JIT compile the model to let Jax know that some arrays may are mutable and some are simple constants. By default, some base python classes play well with JIT but we highly recomed to wrap everything around a Constant or a Variable according to its role in your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeModuleConfig(spark.nn.Config):\n",
    "    foo: int\n",
    "    bar: float = 2.0\n",
    "\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "    config: MyAwesomeModuleConfig\n",
    "\n",
    "    def __init__(self, config: MyAwesomeModuleConfig = None, **kwargs):\n",
    "        super().__init__(config=config, **kwargs)\n",
    "        #self.foo = jnp.array(self.config.foo) <--- # Will throw an error.\n",
    "        self.foo = spark.Constant(jnp.array(self.config.foo))\n",
    "        self.bar = spark.Variable(jnp.array(self.config.bar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, <b>spark.nn.Module</b> is an abstract class and as such it cannot be instantiated if some methods are not defined first. In this case we only need the <b>\\_\\_init\\_\\_</b> and the <b>\\_\\_call\\_\\_</b> methods. However, note that depending on the particular subclass of <b>Module</b> other functions may be necessary.\n",
    "\n",
    "Apart from the configuration class, <b>\\_\\_call\\_\\_</b> is the other strongly typed element in <b><i>Spark</i></b>. The first thing to notice is that <b>\\_\\_call\\_\\_</b> does NOT accept positional arguments only keyword arguments. This keyword arguments must always be typed and the type must always inherit from <b>spark.SparkPayload</b>. Payloads are just wrappers around arrays that helps the internal machinery to know what can connect with what. Every array withing any default payload can be access via <b>my_payload.value</b>. \n",
    "\n",
    "Next, <b>\\_\\_call\\_\\_</b> must always specify what it returns. This is done through the arrow indication <b>\\-\\></b> at the end of the <b>\\_\\_call\\_\\_</b>. The return type is always a TypedDict, that defines the names and the types of each variable that you intend to return after the <b>\\_\\_call\\_\\_</b>. Note that again, all return types must inherit from <b>spark.SparkPayload</b>. \n",
    "\n",
    "Finally, the return of the <b>\\_\\_call\\_\\_</b> is a dictionary that contains all the variables that you specified in TypedDict, with their respective format.\n",
    "This few type hints really provide the internal machinery with guides on what to do under certain circumstances.\n",
    "\n",
    "And this is all typing that you need to do!. No more typing after this if you do not like it!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeOutput(tp.TypedDict):\n",
    "    my_awesome_output: spark.FloatArray\n",
    "\n",
    "class MyAwesomeModuleConfig(spark.nn.Config):\n",
    "    foo: int\n",
    "    bar: float = 2.0\n",
    "\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "    config: MyAwesomeModuleConfig\n",
    "\n",
    "    def __init__(self, config: MyAwesomeModuleConfig = None, **kwargs):\n",
    "        super().__init__(config=config, **kwargs)\n",
    "        self.foo = spark.Constant(jnp.array(self.config.foo))\n",
    "        self.bar = spark.Variable(jnp.array(self.config.bar))\n",
    "\n",
    "    def __call__(self, my_awesome_input: spark.FloatArray) -> MyAwesomeOutput:\n",
    "        awesome_output = self.foo + self.bar + my_awesome_input\n",
    "        return {\n",
    "            'my_awesome_output': spark.FloatArray(awesome_output)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeOutput(tp.TypedDict):\n",
    "    my_awesome_output: spark.FloatArray\n",
    "\n",
    "class MyAwesomeModuleConfig(spark.nn.Config):\n",
    "    foo: int\n",
    "    bar: float = 2.0\n",
    "\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "    config: MyAwesomeModuleConfig\n",
    "\n",
    "    def __init__(self, config: MyAwesomeModuleConfig = None, **kwargs):\n",
    "        super().__init__(config=config, **kwargs)\n",
    "        self.foo = spark.Constant(jnp.array(self.config.foo))\n",
    "\n",
    "    def build(self, input_specs):\n",
    "        mai_spec = input_specs['my_awesome_input']\n",
    "        self.bar = spark.Variable(\n",
    "            mai_spec.payload_type( self.config.bar * jnp.ones(mai_spec.shape) )\n",
    "        )\n",
    "\n",
    "    def __call__(self, my_awesome_input: spark.FloatArray) -> MyAwesomeOutput:\n",
    "        awesome_output = self.foo + self.bar + my_awesome_input\n",
    "        return {\n",
    "            'my_awesome_output': spark.FloatArray(awesome_output)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeOutput(tp.TypedDict):\n",
    "    my_awesome_output: spark.FloatArray\n",
    "\n",
    "class MyAwesomeModuleConfig(spark.nn.Config):\n",
    "    foo: int\n",
    "    bar: float = 2.0\n",
    "\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "    config: MyAwesomeModuleConfig\n",
    "\n",
    "    def __init__(self, config: MyAwesomeModuleConfig = None, **kwargs):\n",
    "        super().__init__(config=config, **kwargs)\n",
    "        self.foo = spark.Constant(jnp.array(self.config.foo))\n",
    "\n",
    "    def build(self, input_specs):\n",
    "        mai_spec = input_specs['my_awesome_input']\n",
    "        self.bar = spark.Variable(\n",
    "            mai_spec.payload_type( self.config.bar * jnp.ones(mai_spec.shape) )\n",
    "        )\n",
    "\n",
    "    def __call__(self, my_awesome_input: spark.FloatArray) -> MyAwesomeOutput:\n",
    "        awesome_output = self.foo + self.bar + my_awesome_input\n",
    "        return {\n",
    "            'my_awesome_output': spark.FloatArray(awesome_output)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      " [4.]\n",
      "\n",
      "Method 2\n",
      " [3. 4. 5. 6. 7.]\n",
      "\n",
      "Method 3\n",
      " [[0. 1.]\n",
      " [2. 3.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "awesome = MyAwesomeModule(foo = 1)\n",
    "my_awesome_input = spark.FloatArray(jnp.array(1))\n",
    "res = awesome(my_awesome_input=my_awesome_input)\n",
    "print(f'Method 1\\n {res['my_awesome_output'].value}\\n')\n",
    "\n",
    "# Method 2\n",
    "awesome_config = MyAwesomeModuleConfig(foo = 1)\n",
    "awesome = MyAwesomeModule(config=awesome_config)\n",
    "my_awesome_input = spark.FloatArray(jnp.arange(5))\n",
    "res = awesome(my_awesome_input=my_awesome_input)\n",
    "print(f'Method 2\\n {res['my_awesome_output'].value}\\n')\n",
    "\n",
    "# Method 3\n",
    "awesome_config = MyAwesomeModuleConfig(foo = 1)\n",
    "awesome = MyAwesomeModule(config=awesome_config, bar=-1)\n",
    "my_awesome_input = spark.FloatArray(jnp.arange(4).reshape(2,2))\n",
    "res = awesome(my_awesome_input=my_awesome_input)\n",
    "print(f'Method 3\\n {res['my_awesome_output'].value}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
