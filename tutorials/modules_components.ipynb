{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial \\#1: Modules and Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Spark</i></b> is intended as a research framework. As such one of the first things that one may want is to add a new element to the systems. <br>\n",
    "<br>\n",
    "Fortunately, adding new components is quite straight forward, although it may be a little confusing the first time. Let's start from the beginning. In <b><i>Spark</i></b>, as in any other major library, every module inherits from <b>spark.nn.Module</b>. This class is at the heart of most of the <b><i>Spark</i></b>'s magic. Depending on the specificity of the module you are trying to build it could make sense to subclass other classes:\n",
    "\n",
    "| Name | Purpose |\n",
    "| --- | --- |\n",
    "| spark.nn.Component | Arbitrary component of a neuron |\n",
    "| spark.nn.somas.Soma | Soma models |\n",
    "| spark.nn.synapses.Synanpses | Synaptic models |\n",
    "| spark.nn.learning_rules.LearningRule | Learning rules |\n",
    "| spark.nn.delays.Delays | Spike delay mechanisms |\n",
    "\n",
    "This list is not exhaustive, but it contains the most important subclasses of <b>spark.nn.Module</b>. \n",
    "\n",
    "<b> REMARK: </b>One important thing to mention before we start is that Python does not strictly enforce typing, however, Spark does rely on typing for a few of our core features. Although some of your code may work without enforcing typing there are a few things that will require it if you like for your components to play nicely with the Spark ecosystem. In general, it is quite straight forward and is only required in a few parts, so if you really despise typing most of your code can still be typeless.\n",
    "\n",
    "For the time being let's just create a simple module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, './..')\n",
    "\n",
    "# Imports\n",
    "import spark\n",
    "import typing as tp\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things to notice is new components are defined in pairs of Module - Configurations, where the Module defines the logic and the Configurations defines all the parameters required to initialize that Module. By convention we simply name these pairs as Module - ModuleConfig. \n",
    "\n",
    "Every time a new module is defined we need to link it to its default configuration. This is done by simply adding  <b>config: ConfigClass</b> to the Module class definition.\n",
    "\n",
    "Another thing to notice at this point is that the signature of all the init methods is common across all modules, it should only accept a configuration class, which by default can be empty and keywords arguments. Similarly, the first thing to do in any <b>\\_\\_init\\_\\_</b> method is to invoke the <b>\\_\\_init\\_\\_</b> method of the super class with the pattern indicated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeModuleConfig(spark.nn.Config):\n",
    "    pass\n",
    "\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "    config: MyAwesomeModuleConfig       # <--- Default configuration class MUST always be indicated\n",
    "\n",
    "    def __init__(self, config: MyAwesomeModuleConfig = None, **kwargs):\n",
    "        super().__init__(config=config, **kwargs)       # <--- super().__init__ should always be invoked first as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you will be wondering where are all your important init arguments go? The answer is the configuration class.<br>\n",
    "<br>\n",
    "Spark modules separate model definition from model logic. This allow us to do some neat tricks on the back and it is extremely useful for reproducibility. Configuration classes should always be typed, this is done using the notation <b>variable_name: variable_type = default_value</b>. \n",
    "\n",
    "All the variables defined inside the configuration class are available in the <b>\\_\\_init\\_\\_</b> method, after calling <b>super().\\_\\_init\\_\\_</b>, under the namespace <b>self.config</b>. \n",
    "\n",
    "Another important thing to notice here is that we cannot store arrays directly. Every array, must be properly wrapper within a <b>spark.Constant</b> or a <b>spark.Variable</b>. This wrappers are necessary when we JIT compile the model to let Jax know that some arrays may are mutable and some are simple constants. By default, some base python classes play well with JIT but we highly recomed to wrap everything around a Constant or a Variable according to its role in your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeModuleConfig(spark.nn.Config):\n",
    "    foo: int\n",
    "    bar: float = 2.0\n",
    "\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "    config: MyAwesomeModuleConfig\n",
    "\n",
    "    def __init__(self, config: MyAwesomeModuleConfig = None, **kwargs):\n",
    "        super().__init__(config=config, **kwargs)\n",
    "        #self.foo = jnp.array(self.config.foo) <--- # Will throw an error later when we start with JIT!.\n",
    "        self.foo = spark.Constant(jnp.array(self.config.foo))\n",
    "        self.bar = spark.Variable(jnp.array(self.config.bar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, <b>spark.nn.Module</b> is an abstract class and as such it cannot be instantiated if some methods are not defined first. In this case we only need the <b>\\_\\_init\\_\\_</b> and the <b>\\_\\_call\\_\\_</b> methods. However, note that depending on the particular subclass of <b>Module</b> other functions may be necessary.\n",
    "\n",
    "Apart from the configuration class, <b>\\_\\_call\\_\\_</b> is the other strongly typed element in <b><i>Spark</i></b>. The first thing to notice is that <b>\\_\\_call\\_\\_</b> does NOT accept positional arguments only keyword arguments (you can still pass then by position if you want but this is not encouraged as we see later on the JIT section). This keyword arguments must always be typed and the type must always inherit from <b>spark.SparkPayload</b>. Payloads are just wrappers around arrays that helps the internal machinery to know what can connect with what. Every array withing any default payload can be access via <b>my_payload.value</b>. \n",
    "\n",
    "Next, <b>\\_\\_call\\_\\_</b> must always specify what it returns. This is done through the arrow indication <b>\\-\\></b> at the end of the <b>\\_\\_call\\_\\_</b>. The return type is always a TypedDict, that defines the names and the types of each variable that you intend to return after the <b>\\_\\_call\\_\\_</b>. Note that, again, all return types must inherit from <b>spark.SparkPayload</b>. \n",
    "\n",
    "Finally, the return of the <b>\\_\\_call\\_\\_</b> is a dictionary that contains all the variables that you specified in TypedDict, with their respective format.\n",
    "This few type hints really provide the internal machinery with guides on what to do under certain circumstances.\n",
    "\n",
    "And this is all typing that you need to do!. No more typing after this if you do not like it!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeOutput(tp.TypedDict):\n",
    "    my_awesome_output: spark.FloatArray\n",
    "\n",
    "class MyAwesomeModuleConfig(spark.nn.Config):\n",
    "    foo: int\n",
    "    bar: float = 2.0\n",
    "\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "    config: MyAwesomeModuleConfig\n",
    "\n",
    "    def __init__(self, config: MyAwesomeModuleConfig = None, **kwargs):\n",
    "        super().__init__(config=config, **kwargs)\n",
    "        self.foo = spark.Constant(jnp.array(self.config.foo))\n",
    "        self.bar = spark.Variable(jnp.array(self.config.bar))\n",
    "\n",
    "    def __call__(self, my_awesome_input: spark.FloatArray) -> MyAwesomeOutput:\n",
    "        awesome_output = self.foo + self.bar + my_awesome_input\n",
    "        return {\n",
    "            'my_awesome_output': spark.FloatArray(awesome_output)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, <b><i>Spark</i></b>, similar to other modern machine learning frameworks, implements lazy initialization. This allows you to build your until you know the other information about the inputs it receives.\n",
    "\n",
    "To access this feature you need to define the <b>build</b> method. This method takes input_specs as argument, which is a dictionary containing specifications of all input variables, like shape and payload type.\n",
    "\n",
    "Note that you can define any other method as any other python object and it will still play nicely with <b><i>Spark</i></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeOutput(tp.TypedDict):\n",
    "\tmy_awesome_output: spark.FloatArray\n",
    "\n",
    "class MyAwesomeModuleConfig(spark.nn.Config):\n",
    "\tfoo: int\n",
    "\tbar: float = 2.0\n",
    "\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "\tconfig: MyAwesomeModuleConfig\n",
    "\n",
    "\tdef __init__(self, config: MyAwesomeModuleConfig = None, **kwargs):\n",
    "\t\tsuper().__init__(config=config, **kwargs)\n",
    "\t\tself.foo = spark.Constant(jnp.array(self.config.foo))\n",
    "\n",
    "\tdef build(self, input_specs: dict[str, spark.InputSpec]):\n",
    "\t\tmai_spec = input_specs['my_awesome_input']\n",
    "\t\tself.bar = spark.Variable(\n",
    "\t\t\tmai_spec.payload_type( self.config.bar * jnp.ones(mai_spec.shape) )\n",
    "\t\t)\n",
    "\n",
    "\tdef boring_non_typed_function(self, a, b, c):\n",
    "\t\treturn a + b + c\n",
    "\n",
    "\tdef __call__(self, my_awesome_input: spark.FloatArray) -> MyAwesomeOutput:\n",
    "\t\tawesome_output = self.boring_non_typed_function(self.foo, self.bar, my_awesome_input)\n",
    "\t\treturn {\n",
    "\t\t\t'my_awesome_output': spark.FloatArray(awesome_output)\n",
    "\t\t}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is everything you need to know to build a custom module.\n",
    "\n",
    "At this point you may be thinking that this pairs of Module - Configuration is going to be really annoying to manage and you could not be more wrong. Informaly, the configuration class is more like a specification than a component of the module. There are several ways to initialize a module. \n",
    "\n",
    "1. Passing a keyword arguments that do not provide a default inside the configuration. <br><br>\n",
    "2. Initialize the configuration and feeding it into the module. Note that it must be provided using the keyword \"config\". <br><br>\n",
    "3. A mixture of both. You can pass some configuration and keyword arguments. Note that in this case, keyworded arguments take preference over variables defined in the configuration class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      " [4.]\n",
      "\n",
      "Method 2\n",
      " [3. 4. 5. 6. 7.]\n",
      "\n",
      "Method 3\n",
      " [[0. 1.]\n",
      " [2. 3.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "awesome = MyAwesomeModule(foo = 1)\n",
    "my_awesome_input = spark.FloatArray(jnp.array(1))\n",
    "res = awesome(my_awesome_input=my_awesome_input)\n",
    "print(f'Method 1\\n {res['my_awesome_output'].value}\\n')\n",
    "\n",
    "# Method 2\n",
    "awesome_config = MyAwesomeModuleConfig(foo = 1)\n",
    "awesome = MyAwesomeModule(config=awesome_config)\n",
    "my_awesome_input = spark.FloatArray(jnp.arange(5))\n",
    "res = awesome(my_awesome_input=my_awesome_input)\n",
    "print(f'Method 2\\n {res['my_awesome_output'].value}\\n')\n",
    "\n",
    "# Method 3\n",
    "awesome_config = MyAwesomeModuleConfig(foo = 1)\n",
    "awesome = MyAwesomeModule(config=awesome_config, bar=-1)\n",
    "my_awesome_input = spark.FloatArray(jnp.arange(4).reshape(2,2))\n",
    "res = awesome(my_awesome_input=my_awesome_input)\n",
    "print(f'Method 3\\n {res['my_awesome_output'].value}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to highlight that the configuration class is significantly more powerful that it was shown until this point.\n",
    "\n",
    "To make full use of the power inside of the configuration class it is highly recomended that you use <b>dc.field</b>; this allows you define significantly more complex configurations and they are particuarly useful to add metadata to variables (Do not lie to me!, you have also meet that horrible programmer that took your code 2 months ago and said that he did not needed to annote the code because he wrote it!.).\n",
    "\n",
    "Internally, there is one metadata field that have an important functional behaviour:\n",
    "1. <b>validators</b>: A list of validators that inherit from <b>spark.validation.ConfigurationValidator</b> that is used to validate the arguments of the configuration class.\n",
    "\n",
    "Additionally, there are a few other metadata fields that hold some special meaning:\n",
    "1. <b>units</b>: Since most of the time, spiking neural network works with parameters that are closely related to real equations/measurements it is ideal to know the expect units. Units are a simple string and its sole purpose is to inform other users of the expected value for some argument. <br><br>\n",
    "2. <b>valid_types</b>: Used for some broadcasting checks. <br><br>\n",
    "3. <b>description</b>: A string description of the purpose of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh now, something went wrong: Attribute \"bar\" must be positive, but got -1.\n",
      "Oh now, something went wrong: Attribute \"bar\" expects types \"float\" or \"int\", but got type \"list\".\n"
     ]
    }
   ],
   "source": [
    "import dataclasses as dc\n",
    "\n",
    "class MyAwesomeModuleConfig(spark.nn.Config):\n",
    "    foo: int\t\t\t\t\t\t\t\t\t\t\t\t# <-- You can still mix it with non-fields.\n",
    "    bar: float = dc.field(\n",
    "        default = 2.0,\n",
    "        metadata = {\n",
    "\t\t\t'units': 'nA', \t\t\t\t\t\t\t\t\t# <-- nano Awesomes.\n",
    "\t\t\t'valid_types': tp.Any,\t\t\t\t\t\t\t# <-- Valid broadcastable to one of your default types.\n",
    "\t\t\t'validators': [\t\t\t\n",
    "                spark.validation.TypeValidator,\t\t\t\t# <-- Extra logic to validate your configuration.\n",
    "                spark.validation.PositiveValidator,\n",
    "\t\t\t], \t\t\n",
    "\t\t\t'description': 'My awesome bar',\t\t\t\t# <-- Text description of this variable.\n",
    "\t\t}\n",
    "\t)\n",
    "    baz: list[int] = dc.field(\n",
    "        default_factory = lambda : [i for i in range(10)]\t# <-- Factories are also useful to define variables.\n",
    "\t)\n",
    "    \n",
    "try:\n",
    "\tMyAwesomeModuleConfig(foo = 1, bar =-1)\n",
    "except (TypeError, ValueError) as e:\n",
    "    print(f'Oh now, something went wrong: {e}')\n",
    "    \n",
    "try:\n",
    "\tMyAwesomeModuleConfig(foo = 1, bar = [2.0])\n",
    "except (TypeError, ValueError) as e:\n",
    "    print(f'Oh now, something went wrong: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important feature of configuration files is that they can be nested. Initializing configurations that hold childs with arguments that do not define a value is still simple, we provide several ways to initialize this configurations:\n",
    "\n",
    "1. Keyworded arguments. <br><br>\n",
    "2. Keyworded dictionaries. <br><br>\n",
    "3. Shared arguments. <br><br>\n",
    "4. Direct initialization.\n",
    "\n",
    "Note that this also work for module initialization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "->Parent:\t1\n",
      "--->Child Bar:\t2\n",
      "--->Child Baz:\t3\n",
      "\n",
      "Method 1\n",
      "->Parent:\t4\n",
      "--->Child Bar:\t5\n",
      "--->Child Baz:\t6\n",
      "\n",
      "Method 3\n",
      "->Parent:\t7\n",
      "--->Child Bar:\t7\n",
      "--->Child Baz:\t7\n",
      "\n",
      "Method 4\n",
      "->Parent:\t8\n",
      "--->Child Bar:\t9\n",
      "--->Child Baz:\t10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dataclasses as dc\n",
    "import typing as tp\n",
    "\n",
    "class ChildConfig(spark.nn.Config):\n",
    "    foo: int\n",
    "    bar: float = 2.0\n",
    "    \n",
    "class ParentConfig(spark.nn.Config):\n",
    "    foo: int\n",
    "    child_bar: ChildConfig\n",
    "    child_baz: ChildConfig = dc.field(\n",
    "        default_factory = lambda **kwargs: ChildConfig(**{**{'foo': 1}, **kwargs})\t\n",
    "\t)\n",
    "    \n",
    "# Method 1\n",
    "config = ParentConfig(foo=1, child_bar__foo=2, child_baz__foo=3)\n",
    "print(f'Method 1\\n->Parent:\\t{config.foo}\\n--->Child Bar:\\t{config.child_bar.foo}\\n--->Child Baz:\\t{config.child_baz.foo}\\n')\n",
    "\n",
    "# Method 2\n",
    "config = ParentConfig(foo=4, child_bar={'foo':5}, child_baz={'foo':6})\n",
    "print(f'Method 1\\n->Parent:\\t{config.foo}\\n--->Child Bar:\\t{config.child_bar.foo}\\n--->Child Baz:\\t{config.child_baz.foo}\\n')\n",
    "\n",
    "# Method 3\n",
    "config = ParentConfig(_s_foo=7)\n",
    "print(f'Method 3\\n->Parent:\\t{config.foo}\\n--->Child Bar:\\t{config.child_bar.foo}\\n--->Child Baz:\\t{config.child_baz.foo}\\n')\n",
    "\n",
    "# Method 4\n",
    "bar = ChildConfig(foo=9)\n",
    "baz = ChildConfig(foo=10)\n",
    "config = ParentConfig(foo=8, child_bar=bar, child_baz=baz)\n",
    "print(f'Method 4\\n->Parent:\\t{config.foo}\\n--->Child Bar:\\t{config.child_bar.foo}\\n--->Child Baz:\\t{config.child_baz.foo}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in order to let spark know about your awesome Module there is one final step: register your module. This is achieved with a simple decorator. This simple step will allow spark to discover your module and use it more robustly. Note this step is optional but required if you want to use your modules within the Graph Editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final module is register as 'my_awesome_module'\n",
    "@spark.register_module\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can set a specific name to the module. This is useful when you encounter a name conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final module is register as 'my_better_awesome_module'\n",
    "@spark.register_module('my_even_more_awesome_module')\n",
    "class MyAwesomeModule(spark.nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=tutorial>\n",
    "Another feature that sometimes is useful is obtaining the class reference directly from the configuration class. This feature is rather useful when you are defining templates of modules. For example, you may want to test whether a one type of synapse is better in a particular scenario, you may not want to define two different neuron models, to swap two set of synapses; in many scenarios, simply swapping configurations will do the trick.\n",
    "\n",
    "If you are following the naming convention of Module - ModuleConfig, then everything is already settle, it is a simple matter of calling <b>config.class_ref</b>. If this naming convention is not of your liking or you are defining custom paths in the registry then you need to set up the reference manually. This is done fairly easy, just add the registry name of your module to the configuration under <b>\\_\\_class\\_ref\\_\\_</b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAwesomeModuleNotFollowingConvention(spark.nn.Config):\n",
    "    __class_ref__ = 'my_even_more_awesome_module'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=tutorial>\n",
    "This should be everything you need to know to start creating new modules!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    /*\n",
       "     This is a highly specific selector for VS Code.\n",
       "     It targets a div whose ID *begins with* \"output-\", which is\n",
       "     the container for cell output. This is a very forceful rule.\n",
       "    */\n",
       "    div.text.tutorial.plox {\n",
       "        font-size: 18px !important;    \n",
       "        text-align: justify;\n",
       "        width: 10ch !important;\n",
       "        \n",
       "        /* DEBUG: Add a red border to see if the selector is being applied. */\n",
       "        border: 2px solid red !important; \n",
       "    }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Ignore this cell, this is just for formatting purposes.\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./style.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"text tutorial plox\">\n",
    "\n",
    "This is the text for my tutorial. I can continue writing paragraphs here, and they will all be styled according to the `div.tutorial` rule. \n",
    "\n",
    "You can even use **Markdown** syntax inside the div, and it will be rendered correctly.\n",
    "\n",
    "* For example, this list will be inside the styled div.\n",
    "* The text will be justified and have a font size of 18px.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
